{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the LangChain directory to sys.path\n",
    "ail_path = Path().resolve().parent\n",
    "if str(ail_path) not in sys.path:\n",
    "    sys.path.append(str(ail_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd\n",
    "import tools.paraphrase\n",
    "from tools.reformat import reformat\n",
    "from tools.score_complete import score_prompt\n",
    "import tools.score_complete\n",
    "import tools.ShortenTool\n",
    "import tools.ExampleTool\n",
    "import tools.jump_iteration\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfairness_categories = ['A', 'CH', 'CR', 'J', 'LAW', 'LTD', 'TER', 'USE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ids = pd.read_csv('../../dataset/claudette_train_merged.tsv', sep='\\t')['document'].unique()\n",
    "val_doc_ids = pd.read_csv('../../dataset/claudette_val_merged.tsv', sep='\\t')['document'].unique()\n",
    "test_doc_ids = pd.read_csv('../../dataset/claudette_test_merged.tsv', sep='\\t')['document'].unique()\n",
    "\n",
    "df = pd.read_csv('../../dataset/claudette_all_merged.tsv', sep='\\t').rename(columns={\"file_name\": \"document\"})\n",
    "df_train = df.loc[df['document'].isin(train_doc_ids)]\n",
    "#df_train_neg = df_train.loc[df_train['label'] == 0]\n",
    "#df_train_pos = df_train.loc[df_train['label'] == 1]\n",
    "df_val = df.loc[df['document'].isin(val_doc_ids)]\n",
    "df_test = df.loc[df['document'].isin(test_doc_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
      "/var/folders/nd/xc3t3y5n57jcb763d6d9rksw0000gn/T/ipykernel_13326/1604170546.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n"
     ]
    }
   ],
   "source": [
    "#df_train_pos_per_cat = {}\n",
    "for category in unfairness_categories:\n",
    "    df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
    "    #df_train.loc[:, category] = df_train_pos.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n",
    "    #df_train_pos_per_cat[category] = df_train.loc[df_train[category] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer_instruction(n_words = 50):\n",
    "    return f'Start your answer with \"yes\" or \"no\" and then justify your response in no more than {n_words} words.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_distribution(df, categories, sample_size, seed=123):\n",
    "    \n",
    "    per_category=int(sample_size/(len(categories)+1))\n",
    "    sample_df = None\n",
    "\n",
    "    for cat in categories:\n",
    "        temp_df = df[df[cat] == 1].sample(n=per_category, random_state=seed)\n",
    "        if type(sample_df) != None:\n",
    "            sample_df = pd.concat([sample_df, temp_df])\n",
    "        else:\n",
    "            sample_df=temp_df\n",
    "\n",
    "    temp_df = df[(df[categories] == 0).all(axis=1)].sample(n=int(sample_size/len(categories)), random_state=seed)\n",
    "    sample_df = pd.concat([sample_df, temp_df])\n",
    "\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_short = sample_equal_distribution(df_train, unfairness_categories, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase = tools.paraphrase.ParaphrasePegasus()\n",
    "shorten = tools.ShortenTool.ShortenTool()\n",
    "examples = tools.ExampleTool.ExampleTool(df_train)\n",
    "jump_iteration = tools.jump_iteration.jump_iteration(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_optimization(intro:str, initial_prompt, df, df_short, model, num_iterations, category):\n",
    "    \"\"\"Optimizes the prompt using an LLM to choose the best edit operation.\"\"\"\n",
    "    ##For this.. online: <current clause, > <Legal Description, > <Examples, >\n",
    "    #optimization_history =  [{'iteration': 1, \"prompt\": part_of_prompt_to_modify, \"score\": 80, \"edit\": \"paraphrase\"}]\n",
    "    optimization_history = []  # Initialize as an empty list attribute\n",
    "    part_of_prompt_to_modify = initial_prompt\n",
    "    for iteration in range(num_iterations):  # Iterate for the specified number of times\n",
    "        # 1. Construct LLM Prompt\n",
    "        if iteration == 0:\n",
    "            # Special prompt for the first iteration (no history)\n",
    "            score_value = score_prompt(intro, examples.added_examples['positive'], examples.added_examples['negative'], part_of_prompt_to_modify, initial_prompt, df_short, category, model, {'p':1, 'r':0, 'l':0}) \n",
    "            optimization_history.append(\n",
    "                {\"iteration\": 0, \"prompt\": part_of_prompt_to_modify, \"edit\": \"inital - no edit\", \"score\": score_value}  # Log 1-based iteration\n",
    "            )\n",
    "    \n",
    "            llm_prompt = (\n",
    "                f\"You are tasked with optimizing a prompt. This is the inital iteration (iteration 0).\\n\" \n",
    "                \"The prompt is scored in a scale of 0 - 100. Your goal is to get a score that is high as possible.\"\n",
    "                \"Full Prompt: <intro><current_clause><legal_description><examples><part_of_prompt_to_modify>\"\n",
    "                f\"<intro> is: {intro}\"\n",
    "                \"<current_clause> is the variable clause to be classified.\"\n",
    "                f\"<legal_description> is: Question helping to classify clauses\"\n",
    "                \"<examples> is: optional, example clauses that are either positive or negative to the classification\"\n",
    "                f\"<part_of_prompt_to_modify> is {part_of_prompt_to_modify}, you will only modify this part\\n\\n\"\n",
    "                f\"Your starting point: {optimization_history}\\n\\n\"\n",
    "                \"To optimize the prompt you can use various actions. These actions alter the prompt.\"\n",
    "                #\"To optimize the prompt you can only use one action. \"\n",
    "                \"Choose the most suitable action to improve the prompt:\\n\"\n",
    "                \"- shorten: Shortens the prompt by removing stopwords from the prompt. Removes filling words.\\n\"\n",
    "                \"- add_positive_example: Include a positive example.\\n\"\n",
    "                \"- add_negative_example: Include a negative example.\\n\"\n",
    "                \"- paraphrase: Rephrase the prompt a slightly bit.\\n\"\n",
    "                \"- reformat: Formates the prompt into bullet points.\\n\"\n",
    "                #\"- grammar_adjustment: Fix any grammatical errors in the prompt.\\n\"\n",
    "                #\"- jump_back_to_iteration: Jump back to a specific iteration. Use the format 'jump_back_to_iteration <number> because: <reasoning>'.\\n\"\n",
    "                \"Your output should be of form: <chosen_action> because: <reasoning>\"\n",
    "            )\n",
    "        else:\n",
    "            # Prompt for subsequent iterations (with history)\n",
    "            llm_prompt = (\n",
    "                f\"You are tasked with optimizing a prompt. This is iteration {iteration + 1}.\\n\"  # Add 1 to display 1-based iteration\n",
    "                \"The prompt is scored in a scale of 0 - 100. Your goal is to get a score that is high as possible.\"\n",
    "                \"Full Prompt: <intro><current_clause><legal_description><examples><part_of_prompt_to_modify>\"\n",
    "                f\"<intro> is: {intro}\"\n",
    "                \"<current_clause> is the variable clause to be classified.\"\n",
    "                f\"<legal_description> is: Question helping to classify clauses\"\n",
    "                \"<examples> is: optional, example clauses that are either positive or negative to the classification\"\n",
    "                f\"<part_of_prompt_to_modify> is {part_of_prompt_to_modify}, you will only modify this part\\n\\n\"\n",
    "                f\"Optimization History: {optimization_history}\\n\\n\"\n",
    "                \"To optimize the prompt you can use various actions. These actions alter the prompt.\"\n",
    "                #\"To optimize the prompt you can only use one action. \"\n",
    "                \"You can use the history to see what was done in previous iterations.\"\n",
    "                \"If an action did not lead to a higher score after two iterations, try a different action.\"\n",
    "                \"Try different actions to get a feeling on what works.\"\n",
    "                \" Choose the most suitable action to further improve the prompt:\\n\"\n",
    "                \"- shorten: Shortens the prompt by removing stopwords from the prompt. Removes filling words. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- add_positive_example: Include a positive example. A positive example is a clause that belongs to the same category being classified (a category of several unfairness categories) because we are classifying for legal unfairness. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- add_negative_example: Includes an example <current_clause> that showes a negative classifiaction. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- remove_positive_example: Remove the existing positive example. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- remove_negative_example: Remove the existing negative example. Your output should be of form: <chosen_action> because: <reasoning>\\n\"                \n",
    "                \"- paraphrase: Rephrase the prompt a slightly bit. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- reformat: Formates the <part_of_prompt_to_modify> into bullet points. Your output should be of form: <chosen_action> because: <reasoning>\\n\"\n",
    "                \"- jump_back_to_iteration: Jump back to a previous iteration. If you realize previous actions led to a decrease in performance, return to an iteration step that has been more promising. Your output should be of form: jump_back_to_iteration <number> because: <reasoning>'.\\n\"\n",
    "                    \n",
    "                #\"- grammar_adjustment: Fix any grammatical errors in the prompt.\\n\"\n",
    "                #\"Your output should be of form: <chosen_action> because: <reasoning>\"\n",
    "            )\n",
    "\n",
    "        print(f\"This is iteration {iteration + 1}\")\n",
    "        # 2. Get LLM's Decision\n",
    "        message = HumanMessage(content=llm_prompt)\n",
    "        response = model.invoke([message])\n",
    "        print(f\"Complete response: {response}\")\n",
    "        chosen_action = response.content.lower().split()[0]\n",
    "        #print(\"Response: \" + response)\n",
    "        print(\"Chosen Action: \" + chosen_action)\n",
    "\n",
    "        #if iteration == 1:\n",
    "        #    chosen_action = \"jump_back_to_iteration\"\n",
    "        #    iteration_number = 0\n",
    "        \n",
    "        # 3. Apply Chosen Action\n",
    "        for i in range(2):\n",
    "            if chosen_action == \"shorten\":\n",
    "                part_of_prompt_to_modify = shorten.shorten_prompt(part_of_prompt_to_modify)\n",
    "                break\n",
    "            elif chosen_action == \"add_positive_example\":\n",
    "                examples.add_positive_example(part_of_prompt_to_modify,category)\n",
    "                break\n",
    "            elif chosen_action == \"add_negative_example\":\n",
    "                examples.add_negative_example(part_of_prompt_to_modify,category)\n",
    "                break\n",
    "            elif chosen_action == \"remove_positive_example\":\n",
    "                examples.remove_positive_example(part_of_prompt_to_modify,category)\n",
    "                break\n",
    "            elif chosen_action == \"remove_negative_example\":\n",
    "                examples.remove_negative_example(part_of_prompt_to_modify,category)\n",
    "                break            \n",
    "            elif chosen_action == \"paraphrase\":\n",
    "                part_of_prompt_to_modify = paraphrase.paraphrase_pegasus(part_of_prompt_to_modify)\n",
    "                break\n",
    "            elif chosen_action == \"reformat\":\n",
    "                part_of_prompt_to_modify = reformat(part_of_prompt_to_modify)\n",
    "                break\n",
    "            elif chosen_action == \"jump_back_to_iteration\":\n",
    "                print(\"\\n\\n\")\n",
    "                match = re.search(r'jump_back_to_iteration (\\d+)', response.content.lower())\n",
    "                if match:\n",
    "                    iteration_number = int(match.group(1))\n",
    "                #print(chosen_action, iteration_number)\n",
    "                part_of_prompt_to_modify = jump_iteration.jump_back_to_iteration(iteration_number, optimization_history)\n",
    "                print(part_of_prompt_to_modify)\n",
    "                print(\"\\n\\n\")\n",
    "                break\n",
    "            #elif chosen_action == \"grammar_adjustment\":\n",
    "            #    grammarAdjustment = GrammarAdjustment()\n",
    "            #    part_of_prompt_to_modify = grammarAdjustment.grammar_adjustment(current_prompt)\n",
    "            else:\n",
    "                keywords = [\n",
    "                    \"shorten\", \n",
    "                    \"add_positive_example\", \n",
    "                    \"add_negative_example\", \n",
    "                    \"remove_positive_example\", \n",
    "                    \"remove_negative_example\", \n",
    "                    \"paraphrase\",  \n",
    "                    \"reformat\",\n",
    "                    \"jump_back_to_iteration\"\n",
    "                ]\n",
    "                \n",
    "                for keyword in keywords:\n",
    "                    if keyword in response.content.lower():\n",
    "                        chosen_action = keyword \n",
    "                        break\n",
    "                    else:\n",
    "                        chosen_action = \"\"\n",
    "                if chosen_action == \"\":        \n",
    "                    raise ValueError(f\"Invalid action chosen by LLM: {chosen_action}\")\n",
    "\n",
    "        # 4. Score and Log (using self.optimization_history)\n",
    "        score_value = score_prompt(intro, examples.added_examples['positive'], examples.added_examples['negative'], part_of_prompt_to_modify, initial_prompt, df_short, category, model, {'p':1, 'r':0, 'l':0}) # + legal description, df_test, category, intro \n",
    "        \n",
    "        optimization_history.append(\n",
    "            {\"iteration\": iteration + 1, \"prompt\": part_of_prompt_to_modify, \"edit\": chosen_action, \"score\": score_value}  # Log 1-based iteration\n",
    "        )\n",
    "\n",
    "    return optimization_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 54.76190476190476\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 1\n",
      "Complete response: content='paraphrase because: The initial prompt is a bit verbose and could be rephrased to be more concise and clear, which might help improve the score.' response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 333, 'total_tokens': 367, 'completion_time': 0.100878134, 'prompt_time': 0.038262322, 'queue_time': None, 'total_time': 0.139140456}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-e15c112c-29ce-41d1-94e4-2948f858163e-0' usage_metadata={'input_tokens': 333, 'output_tokens': 34, 'total_tokens': 367}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start your answer with 'yes' or 'no' and then justify your response in no more than 50 words. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 36.66666666666667\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 2\n",
      "Complete response: content=\"paraphrase because: The previous paraphrase action led to a decrease in score, so I'll try a different paraphrase to see if it improves the score.\" response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 659, 'total_tokens': 693, 'completion_time': 0.098008842, 'prompt_time': 0.066854416, 'queue_time': None, 'total_time': 0.164863258}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-c89cfe52-e541-47d4-8d13-2cd90d9ea37d-0' usage_metadata={'input_tokens': 659, 'output_tokens': 34, 'total_tokens': 693}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: If you want to justify your response, start it with 'yes' or 'no' and end it with 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 17.391304347826086\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 3\n",
      "Complete response: content='paraphrase because: The previous paraphrasing attempts did not lead to an increase in score, but I want to try again with a fresh perspective. Maybe a slight rewording can help clarify the prompt and improve the score.' response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 701, 'total_tokens': 748, 'completion_time': 0.137526111, 'prompt_time': 0.076759482, 'queue_time': None, 'total_time': 0.214285593}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-2290c251-2b41-48d5-b325-ac409dee5c23-0' usage_metadata={'input_tokens': 701, 'output_tokens': 47, 'total_tokens': 748}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start your response with 'yes' or 'no' and end it with 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 17.391304347826086\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 4\n",
      "Complete response: content=\"paraphrase because: The previous paraphrasing attempts did not lead to a significant improvement, but it's still worth trying to rephrase the prompt to make it more concise and clear.\" response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 750, 'total_tokens': 788, 'completion_time': 0.117294424, 'prompt_time': 0.077501035, 'queue_time': None, 'total_time': 0.194795459}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None} id='run-f90603b6-d16e-49b8-9805-7c1f0f621fec-0' usage_metadata={'input_tokens': 750, 'output_tokens': 38, 'total_tokens': 788}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Begin your response with 'yes' or 'no' and end it with 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 17.391304347826086\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 5\n",
      "Complete response: content=\"paraphrase because: The previous iterations' scores have been stuck at 17.391304347826086, and paraphrasing has been the most consistent action so far. I'll try to rephrase the prompt again to see if it can break the plateau.\" response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 799, 'total_tokens': 853, 'completion_time': 0.165067514, 'prompt_time': 0.080386449, 'queue_time': None, 'total_time': 0.245453963}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_f128cfcd6c', 'finish_reason': 'stop', 'logprobs': None} id='run-4f997180-ad59-40a4-8780-c95bb4018a58-0' usage_metadata={'input_tokens': 799, 'output_tokens': 54, 'total_tokens': 853}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Begin your response with 'yes' or 'no', and end it with 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 68.33333333333333\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 6\n",
      "Complete response: content='reformat because: The current prompt is concise and clear, but formatting the part of the prompt to modify into bullet points could make it more visually appealing and easier to understand, potentially leading to a higher score.' response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 834, 'total_tokens': 877, 'completion_time': 0.125932658, 'prompt_time': 0.091036418, 'queue_time': None, 'total_time': 0.21696907599999998}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-b6ec704b-4011-4682-9c08-3054d116664c-0' usage_metadata={'input_tokens': 834, 'output_tokens': 43, 'total_tokens': 877}\n",
      "Chosen Action: reformat\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 75.64102564102564\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 7\n",
      "Complete response: content='paraphrase because: The current prompt is quite concise, and the score is already high. A slight rephrasing might help to further improve the clarity and coherence of the prompt, which could lead to an even higher score.' response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 874, 'total_tokens': 921, 'completion_time': 0.142739493, 'prompt_time': 0.141372764, 'queue_time': None, 'total_time': 0.284112257}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None} id='run-c09d8747-54f0-4f7c-b747-1389659c46f9-0' usage_metadata={'input_tokens': 874, 'output_tokens': 47, 'total_tokens': 921}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt:  - Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 73.73271889400921\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 8\n",
      "Complete response: content='paraphrase because: The current prompt is concise, but a slight rephrasing might help to make it even clearer and more direct.' response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 912, 'total_tokens': 941, 'completion_time': 0.084593351, 'prompt_time': 0.094212629, 'queue_time': None, 'total_time': 0.17880598}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-87ccac85-0235-412d-8e9e-8aad0abdb6ab-0' usage_metadata={'input_tokens': 912, 'output_tokens': 29, 'total_tokens': 941}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 73.73271889400921\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 9\n",
      "Complete response: content='paraphrase because: The current prompt is concise and clear, but a slight rephrasing might help to further improve the score. The goal is to make the prompt more intuitive and easy to understand, which could lead to a higher score.' response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 952, 'total_tokens': 1002, 'completion_time': 0.15167644, 'prompt_time': 0.120163963, 'queue_time': None, 'total_time': 0.271840403}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-816ecd5f-e58e-4e38-8408-b986ec829994-0' usage_metadata={'input_tokens': 952, 'output_tokens': 50, 'total_tokens': 1002}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 73.73271889400921\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 10\n",
      "Complete response: content='After analyzing the optimization history, I notice that the score has plateaued in the last three iterations. To break this plateau, I\\'ll try a different action.\\n\\nI choose: paraphrase because: The current prompt is concise, but a slight rephrasing might help to make it more clear and effective.\\n\\nNew prompt: \"Please respond with \\'yes\\' or \\'no\\'.\"' response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 992, 'total_tokens': 1069, 'completion_time': 0.228789967, 'prompt_time': 0.125316883, 'queue_time': None, 'total_time': 0.35410685}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_f128cfcd6c', 'finish_reason': 'stop', 'logprobs': None} id='run-490afa86-9e79-479b-9d0a-7027c46c6d31-0' usage_metadata={'input_tokens': 992, 'output_tokens': 77, 'total_tokens': 1069}\n",
      "Chosen Action: after\n",
      "Starting to paraphrase the following prompt: Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 73.73271889400921\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 11\n",
      "Complete response: content='paraphrase because: The current prompt is already concise and clear, and paraphrasing can help to slightly rephrase the prompt to make it more natural and fluent, which may lead to a higher score.' response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 1032, 'total_tokens': 1074, 'completion_time': 0.125139185, 'prompt_time': 0.131912589, 'queue_time': None, 'total_time': 0.257051774}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None} id='run-c5f854b5-6847-424f-a4c2-4040da245039-0' usage_metadata={'input_tokens': 1032, 'output_tokens': 42, 'total_tokens': 1074}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 73.73271889400921\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 12\n",
      "Complete response: content='paraphrase because: The current prompt has been stagnant for several iterations, and a slight rephrasing might help to improve the score.' response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1072, 'total_tokens': 1101, 'completion_time': 0.083645747, 'prompt_time': 0.124872579, 'queue_time': None, 'total_time': 0.208518326}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-fd317e78-a425-491f-a44f-f449a5399549-0' usage_metadata={'input_tokens': 1072, 'output_tokens': 29, 'total_tokens': 1101}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 68.33333333333333\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 13\n",
      "Complete response: content=\"paraphrase because: The current prompt is simple and concise, but a slight rephrasing might help to make it even clearer and more effective. I'll try to rephrase the prompt to see if it can lead to a higher score.\" response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 1114, 'total_tokens': 1164, 'completion_time': 0.149625021, 'prompt_time': 0.177791982, 'queue_time': None, 'total_time': 0.327417003}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None} id='run-51f21d51-6504-4557-9284-1cd3b8c29dac-0' usage_metadata={'input_tokens': 1114, 'output_tokens': 50, 'total_tokens': 1164}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with either 'yes' or 'no'. \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 60.416666666666664\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 14\n",
      "Complete response: content='After analyzing the optimization history, I notice that the score has been fluctuating, and the current score is lower than the highest score achieved in iteration 6. I decide to try a different approach to improve the prompt.\\n\\nI choose to `reformat` because: the current prompt is concise, but formatting the `<part_of_prompt_to_modify>` into bullet points might make it more readable and easier to understand, which could lead to a higher score.\\n\\nMy output is: `reformat because: making the prompt more readable might improve the score`.' response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 1155, 'total_tokens': 1265, 'completion_time': 0.340061143, 'prompt_time': 0.17622101, 'queue_time': None, 'total_time': 0.516282153}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-cad55eb9-e22b-4d80-9d6b-957122d37748-0' usage_metadata={'input_tokens': 1155, 'output_tokens': 110, 'total_tokens': 1265}\n",
      "Chosen Action: after\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 78.16091954022988\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 15\n",
      "Complete response: content='paraphrase because: The current prompt is already concise and clear, and paraphrasing can help to slightly rephrase the prompt to make it more natural and fluent, which may lead to a higher score.' response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 1196, 'total_tokens': 1238, 'completion_time': 0.124428667, 'prompt_time': 0.13490359, 'queue_time': None, 'total_time': 0.259332257}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None} id='run-cf991185-be13-43ec-8aba-40e0646348db-0' usage_metadata={'input_tokens': 1196, 'output_tokens': 42, 'total_tokens': 1238}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt:  - Start with \"yes\" or \"no\". \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 60.416666666666664\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 16\n",
      "Complete response: content=\"After analyzing the optimization history, I notice that the score has been fluctuating, and the current score is not the highest. I also see that the last few iterations didn't lead to significant improvements.\\n\\nI decide to try a different approach. Since the prompt is quite short and concise, I'll try to add a positive example to provide more context and guidance for the classification task.\\n\\nMy chosen action is: add_positive_example because: Adding a positive example can help clarify the classification task and provide a concrete instance of what constitutes a positive classification, which may lead to a higher score.\" response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 1234, 'total_tokens': 1351, 'completion_time': 0.347055633, 'prompt_time': 0.213049579, 'queue_time': None, 'total_time': 0.560105212}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-51f4346d-cb52-475f-89d4-8cc5a4abfff6-0' usage_metadata={'input_tokens': 1234, 'output_tokens': 117, 'total_tokens': 1351}\n",
      "Chosen Action: after\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 70.76923076923077\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 17\n",
      "Complete response: content='I choose: paraphrase because: The current prompt is quite simple and direct, and paraphrasing it might help to make it more clear and concise, which could lead to a higher score.' response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 1274, 'total_tokens': 1313, 'completion_time': 0.118628165, 'prompt_time': 0.129450648, 'queue_time': None, 'total_time': 0.248078813}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None} id='run-9860bb3f-de1a-4ce5-9f37-f55efb974157-0' usage_metadata={'input_tokens': 1274, 'output_tokens': 39, 'total_tokens': 1313}\n",
      "Chosen Action: i\n",
      "Starting to paraphrase the following prompt: Start with \"yes\" or \"no\". \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 70.76923076923077\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 18\n",
      "Complete response: content='I choose: add_positive_example because: I want to provide more context and help the model understand what a positive classification looks like, which can lead to a higher score.\\n\\nThe new prompt will be: \\nConsider the following online terms of service clause:<current_clause> is the variable clause to be classified.<legal_description> is: Question helping to classify clauses<examples> is: For example, \"The company reserves the right to terminate your account at any time without notice.\" is a clause that is classified as unfair., Start with \"yes\" or \"no\".' response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 1314, 'total_tokens': 1428, 'completion_time': 0.347531344, 'prompt_time': 0.223655897, 'queue_time': None, 'total_time': 0.5711872410000001}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None} id='run-dd0bd5c1-3b6c-47d2-bb92-fc9c8ec42932-0' usage_metadata={'input_tokens': 1314, 'output_tokens': 114, 'total_tokens': 1428}\n",
      "Chosen Action: i\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 70.76923076923077\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 19\n",
      "Complete response: content=\"I choose: paraphrase because: The current prompt is simple and concise, but a slight rephrasing might help to improve the score. I'll try to rephrase the prompt to make it more clear and direct.\" response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 1354, 'total_tokens': 1399, 'completion_time': 0.131079845, 'prompt_time': 0.358031493, 'queue_time': None, 'total_time': 0.48911133799999995}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None} id='run-c33b344e-468e-4d49-b505-3ef1cd8f1c8e-0' usage_metadata={'input_tokens': 1354, 'output_tokens': 45, 'total_tokens': 1399}\n",
      "Chosen Action: i\n",
      "Starting to paraphrase the following prompt: Start with \"yes\" or \"no\". \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 70.76923076923077\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "This is iteration 20\n",
      "Complete response: content='paraphrase because: The current prompt has been stable for a few iterations, and a slight rephrasing might help to further improve the score.' response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 1394, 'total_tokens': 1425, 'completion_time': 0.092431425, 'prompt_time': 0.180051899, 'queue_time': None, 'total_time': 0.27248332399999997}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None} id='run-0f2751c9-2f7a-43c6-8fe8-543893486f9c-0' usage_metadata={'input_tokens': 1394, 'output_tokens': 31, 'total_tokens': 1425}\n",
      "Chosen Action: paraphrase\n",
      "Starting to paraphrase the following prompt: Start with \"yes\" or \"no\". \n",
      " Using Pegasus:\n",
      "Start getting performance score\n",
      "Start analyzing predicitions:\n",
      "...................\n",
      "Performance score is: 70.76923076923077\n",
      "Start getting length score\n",
      "Lenght score is: 100\n",
      "[\n",
      "  {\n",
      "    \"iteration\": 0,\n",
      "    \"prompt\": \"Start your answer with 'yes' or 'no' and then justify your response in no more than 50 words.\",\n",
      "    \"edit\": \"inital - no edit\",\n",
      "    \"score\": 54.76190476190476\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 1,\n",
      "    \"prompt\": \"If you want to justify your response, start it with 'yes' or 'no' and end it with 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 36.66666666666667\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 2,\n",
      "    \"prompt\": \"Start your response with 'yes' or 'no' and end it with 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 17.391304347826086\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 3,\n",
      "    \"prompt\": \"Begin your response with 'yes' or 'no' and end it with 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 17.391304347826086\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 4,\n",
      "    \"prompt\": \"Begin your response with 'yes' or 'no', and end it with 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 17.391304347826086\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 5,\n",
      "    \"prompt\": \"Begin your response with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 68.33333333333333\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 6,\n",
      "    \"prompt\": \" - Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"reformat\",\n",
      "    \"score\": 75.64102564102564\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 7,\n",
      "    \"prompt\": \"Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 73.73271889400921\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 8,\n",
      "    \"prompt\": \"Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 73.73271889400921\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 9,\n",
      "    \"prompt\": \"Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 73.73271889400921\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 10,\n",
      "    \"prompt\": \"Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 73.73271889400921\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 11,\n",
      "    \"prompt\": \"Start with 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 73.73271889400921\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 12,\n",
      "    \"prompt\": \"Start with either 'yes' or 'no'.\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 68.33333333333333\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 13,\n",
      "    \"prompt\": \"Start with either \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 60.416666666666664\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 14,\n",
      "    \"prompt\": \" - Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"reformat\",\n",
      "    \"score\": 78.16091954022988\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 15,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 60.416666666666664\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 16,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"add_positive_example\",\n",
      "    \"score\": 70.76923076923077\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 17,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 70.76923076923077\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 18,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"add_positive_example\",\n",
      "    \"score\": 70.76923076923077\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 19,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 70.76923076923077\n",
      "  },\n",
      "  {\n",
      "    \"iteration\": 20,\n",
      "    \"prompt\": \"Start with \\\"yes\\\" or \\\"no\\\".\",\n",
      "    \"edit\": \"paraphrase\",\n",
      "    \"score\": 70.76923076923077\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Define your model (using your actual API key/setup)\n",
    "model = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=\"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\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Initial prompt to be optimized\n",
    "\n",
    "#initial_prompt = \"Consider the following online terms of service clause: 'websites & communications terms of use'. Does this clause describe an arbitration dispute resolution process that is not fully optional to the consumer? Begin your answer with 'yes' or 'no' and then justify your response in no more than 50 words. For example, consider these example clauses: <positive, if you are not a consumer in the eea , the exclusive place of jurisdiction for all disputes arising from or in connection with this agreement is san francisco county , california , or the united states district court for the northern district of california and our dispute will bedetermined under california law .'>, <negative, you are prohibited from using any services or facilities provided in connection with this service to compromise security or tamper with system resources and/or accounts .>\"\n",
    "unfairness_categories = ['A', 'CH', 'CR', 'J', 'LAW', 'LTD', 'TER', 'USE']\n",
    "initial_prompt = \"Start your answer with 'yes' or 'no' and then justify your response in no more than 50 words.\"\n",
    "intro_prompt = 'Consider the following online terms of service clause:'\n",
    "#cat_results = {}\n",
    "#df_train_sample = sample_equal_distribution(df_train, unfairness_categories, 100, 123)\n",
    "#score_set_df = sample_equal_distribution(df_train, unfairness_categories, 25, 234)\n",
    "\n",
    "optimized_history = run_prompt_optimization(intro_prompt, initial_prompt, df_train, df_train_short, model, 20,unfairness_categories[0])\n",
    "optimized_prompt = optimized_history[-1][\"prompt\"]\n",
    "    \n",
    "\n",
    "print(json.dumps(optimized_history, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intrinsic bias towards \"paraphrase\" "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
