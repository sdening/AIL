{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install langchain langchain_huggingface torch transformers langchain_groq"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unfairness_categories = ['A', 'CH', 'CR', 'J', 'LAW', 'LTD', 'TER', 'USE']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_doc_ids = pd.read_csv('dataset/claudette_train_merged.tsv', sep='\\t')['document'].unique()\n","val_doc_ids = pd.read_csv('dataset/claudette_val_merged.tsv', sep='\\t')['document'].unique()\n","test_doc_ids = pd.read_csv('dataset/claudette_test_merged.tsv', sep='\\t')['document'].unique()\n","\n","df = pd.read_csv('dataset/claudette_all_merged.tsv', sep='\\t').rename(columns={\"file_name\": \"document\"})\n","df_train = df.loc[df['document'].isin(train_doc_ids)]\n","df_train_neg = df_train.loc[df_train['label'] == 0]\n","df_train_pos = df_train.loc[df_train['label'] == 1]\n","df_val = df.loc[df['document'].isin(val_doc_ids)]\n","df_test = df.loc[df['document'].isin(test_doc_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train_pos_per_cat = {}\n","for category in unfairness_categories:\n","    df_train.loc[:, category] = df_train.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n","    df_train_pos.loc[:, category] = df_train_pos.loc[:, 'label_type'].apply(lambda cats: int(category.upper() in cats))\n","    df_train_pos_per_cat[category] = df_train.loc[df_train[category] == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_answer_instruction(n_words = 50):\n","    return f'Start your answer with \"yes\" or \"no\" and then justify your response in no more than {n_words} words.' "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_example(filtered_set, cat):\n","    \n","    example = filtered_set['text'].sample(n=1).iloc[0]\n","\n","    return example"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_positive_example(part_of_the_prompt: str, cat: str) -> str:\n","    \n","    \"\"\"\n","    Add positive example to the prompt.\n","    Maintains a list of added examples for possible removal.\n","\n","    Args:\n","        part_of_the_prompt (str): The part of the prompt to which the example will be added or from which it will be removed.\n","        cat (str): The category column name in the test set.\n","        example_type (str): The type of example to find ('positive' or 'negative').\n","        mode (str): Mode of operation - 'add' to add an example, 'remove' to remove an example.\n","\n","    Returns:\n","        str: The updated part of the prompt with example clauses\n","    \"\"\"\n","    \n","    global added_examples\n","    \n","    example_type = \"positive\"\n","    mode = \"add\"\n","    \n","    \n","    if mode == \"add\":\n","        if example_type == \"positive\":\n","            filtered_set = df_train[df_train[cat] == 1]\n","        elif example_type == \"negative\":\n","            filtered_set = df_train[df_train[cat] == 0]\n","\n","        if filtered_set.empty:\n","            return \"No positive clause found of the category in data set.\"\n","        \n","        sampled_example = sample_example(filtered_set, cat)\n","            \n","        added_examples[example_type].append(sampled_example)\n","        \n","        \n","        examples_text = \"\"\n","        for example_type, examples in added_examples.items():\n","            for example in examples:\n","                examples_text += f\"For example, consider this clause of the same category: \\\"{example}\\\"\"\n","        \n","        return examples_text + \". \" + part_of_the_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_negative_example(part_of_the_prompt: str, cat: str) -> str:\n","    \n","    \"\"\"\n","    Add negative example to the prompt.\n","    Maintains a list of added examples for possible removal.\n","\n","    Args:\n","        part_of_the_prompt (str): The part of the prompt to which the example will be added or from which it will be removed.\n","        cat (str): The category column name in the test set.\n","        example_type (str): The type of example to find ('positive' or 'negative').\n","        mode (str): Mode of operation - 'add' to add an example, 'remove' to remove an example.\n","\n","    Returns:\n","        str: The updated part of the prompt with example clauses\n","    \"\"\"\n","    \n","    global added_examples\n","    \n","    example_type = \"negative\"\n","    mode = \"add\"\n","    \n","    \n","    if mode == \"add\":\n","        if example_type == \"positive\":\n","            filtered_set = df_train[df_train[cat] == 1]\n","        elif example_type == \"negative\":\n","            filtered_set = df_train[df_train[cat] == 0]\n","\n","        if filtered_set.empty:\n","            return \"No negative clause found of the category in data set.\"\n","        \n","        sampled_example = sample_example(filtered_set, cat)\n","            \n","        added_examples[example_type].append(sampled_example)\n","\n","        \n","        examples_text = \"\"\n","        for example_type, examples in added_examples.items():\n","            for example in examples:\n","                examples_text += f\"For example, consider this clause which is not of this category: \\\"{example}\\\"\"\n","        \n","        return examples_text + \". \" + part_of_the_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" def remove_positive_example(part_of_the_prompt: str, cat: str) -> str:\n","    \n","    \"\"\"\n","    Remove positive example from the prompt.\n","    Maintained a list of added examples for possible removal.\n","\n","    Args:\n","        part_of_the_prompt (str): The part of the prompt to which the example will be added or from which it will be removed.\n","        cat (str): The category column name in the test set.\n","        example_type (str): The type of example to find ('positive' or 'negative').\n","        mode (str): Mode of operation - 'add' to add an example, 'remove' to remove an example.\n","\n","    Returns:\n","        str: The updated part of the prompt with example clauses\n","    \"\"\"\n","    \n","    global added_examples\n","    \n","    example_type = \"positive\"\n","    mode = \"remove\"\n","    \n","    \n","    if mode == \"remove\":\n","        if not added_examples[example_type]:\n","            print(f\"No {example_type} example to remove. \\n\" )\n","            examples_text = \"\"\n","            for example_type, examples in added_examples.items():\n","                for example in examples:\n","                    examples_text += f\"For example, consider this clause of the same category: \\\"{example}\\\"\"\n","\n","            return examples_text + part_of_the_prompt  \n","        \n","        removed_example = added_examples[example_type].pop(0)\n","        \n","        examples_text = \"\"\n","        for example_type, examples in added_examples.items():\n","            for example in examples:\n","                examples_text += f\"For example, consider this clause of the same category: \\\"{example}\\\"\"\n","        \n","        return examples_text + part_of_the_prompt "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_negative_example(part_of_the_prompt: str, cat: str) -> str:\n","    \n","    \"\"\"\n","    Remove negative example from the prompt.\n","    Maintained a list of added examples for possible removal.\n","\n","    Args:\n","        part_of_the_prompt (str): The part of the prompt to which the example will be added or from which it will be removed.\n","        cat (str): The category column name in the test set.\n","        example_type (str): The type of example to find ('positive' or 'negative').\n","        mode (str): Mode of operation - 'add' to add an example, 'remove' to remove an example.\n","\n","    Returns:\n","        str: The updated part of the prompt with example clauses\n","    \"\"\"\n","    \n","    global added_examples\n","    \n","    example_type = \"negative\"\n","    mode = \"remove\"\n","    \n","    \n","    if mode == \"remove\":\n","        if not added_examples[example_type]:\n","            print(f\"No {example_type} example to remove. \\n\" )\n","            examples_text = \"\"\n","            for example_type, examples in added_examples.items():\n","                for example in examples:\n","                    examples_text += f\" For example, consider this clause which is not of this category: \\\"{example}\\\"\"\n","\n","            return examples_text + part_of_the_prompt \n","        \n","        removed_example = added_examples[example_type].pop(0)\n","        \n","        examples_text = \"\"\n","        for example_type, examples in added_examples.items():\n","            for example in examples:\n","                examples_text += f\" For example, consider this clause which is not of this category: \\\"{example}\\\"\"\n","        \n","        return examples_text + part_of_the_prompt "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","from langchain.tools import BaseTool, StructuredTool, tool\n","from langchain_huggingface.llms import HuggingFacePipeline\n","from transformers import pipeline, PegasusForConditionalGeneration, PegasusTokenizer\n","import torch\n","\n","def paraphrase_huggingface(inital_prompt: str, pipe: pipeline) -> str:\n","    \"\"\"\n","    Description = \"Generates a variation of the input text while keeping the semantic meaning using HuggingFace pipeline.\"\n","    \"\"\"\n","    name = \"paraphrase_huggingface\"\n","\n","    hf = HuggingFacePipeline(pipeline=pipe)\n","\n","    # Prepare the prompt template for the paraphrasing task\n","    template = \"\"\"Generate a variation of the input text while keeping the semantic meaning: \\n Input:{text} \\n Output: \"\"\"\n","\n","    # Create the chain and run it\n","    prompt = PromptTemplate.from_template(template)\n","    chain = prompt | hf.bind(skip_prompt=True)\n","    print(f\"Starting to paraphrase the following prompt: {inital_prompt}\\n Using {pipeline} as pipeline:\")\n","    result = chain.invoke({\"text\": inital_prompt})\n","    \n","    return result\n","\n","class ParaphrasePegasus():\n","    def __init__(self, model_name: str = 'tuner007/pegasus_paraphrase') -> None:    \n","        self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.para_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","        self.para_model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.torch_device)\n","\n","    def paraphrase_pegasus(self, inital_prompt: str, num_beams: int = 10, num_return_sequences: int = 1) -> str:\n","        \"\"\"\n","        Description: \"Generates a variation of the input text while keeping the semantic meaning using Pegasus model.\"\n","        \"\"\"\n","        # Paraphrase pipeline\n","        print(f\"Starting to paraphrase the following prompt: {inital_prompt} \\n Using Pegasus:\")\n","        batch = self.para_tokenizer([inital_prompt], truncation=True, padding='longest', max_length=60, return_tensors=\"pt\").to(self.torch_device)\n","        translated = self.para_model.generate(**batch, max_length=60, num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5, do_sample=True)\n","        tgt_text = self.para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n","        \n","        return tgt_text[0]  \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, T5ForConditionalGeneration\n","import torch\n","\n","class GrammarAdjustment():\n","    def __init__(self) -> None:\n","        self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n","        self.model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\")\n","\n","    def grammatic_adjustment(self, inital_prompt: str):\n","        gram_check_promtp = f\"Fix grammatical errors in this sentence: {inital_prompt}\"\n","        input_ids = self.tokenizer(gram_check_promtp, return_tensors=\"pt\").input_ids\n","        outputs = self.model.generate(input_ids, max_length=256)\n","        edited_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return edited_text\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_prompt_optimization(\n","    part_of_prompt_to_modify: str, model, num_iterations: int = 3, category=category\n",") -> List[Dict]:\n","    \"\"\"Optimizes the prompt using an LLM to choose the best edit operation.\"\"\"\n","    ## For this.. online: <current clause, > <Legal Description, > <Examples, >\n","    ## [{'iteration': 1, \"prompt\": \"<intro> <current_clause><legal description> <examples> <part_of_prompt_to_modify>\", \"score\": 80}]\n","    self.optimization_history = []  # Initialize as an empty list attribute\n","    current_part_of_prompt_to_modify = part_of_prompt_to_modify\n","\n","    for iteration in range(num_iterations):  # Iterate for the specified number of times\n","        # 1. Construct LLM Prompt\n","        if iteration == 0:\n","            # Special prompt for the first iteration (no history)\n","            llm_prompt = (\n","                f\"You are tasked with optimizing a prompt. This is iteration {iteration + 1}.\\n\" \n","                \"Full Prompt: <intro><current_clause><legal_description><examples><part_of_prompt_to_modify>\"\n","                \"<intro> is: Consider the following online terms of service clause:\"\n","                \"current_clause> is the variable clause to be classified.\"\n","                \"<legal_description> is: question helping to classify clauses\"\n","                \"<examples> is: optional, example clauses that are either positive or negative to the classification\"\n","                f\"<part_of_prompt_to_modify> is {current_part_of_prompt_to_modify}, only modify this part\\n\\n\"\n","                \n","                \"Choose the most suitable action to improve the prompt:\\n\"\n","                \"- shorten: Remove unnecessary details.\\n\"\n","                \"- add_positive_example: Include a positive example.\\n\"\n","                \"- add_negative_example: Include a negative example.\\n\"\n","                \"- paraphrase: Rephrase the prompt to be more clear and concise.\\n\"\n","                \"- reformat: Change the structure or formatting of the prompt.\\n\"\n","                \"- grammar_adjustment: Fix any grammatical errors in the prompt.\\n\"\n","            )\n","        else:\n","            # Prompt for subsequent iterations (with history)\n","            llm_prompt = (\n","                f\"You are tasked with optimizing a prompt. This is iteration {iteration + 1}.\\n\"  # Add 1 to display 1-based iteration\n","                \"Full Prompt: <intro><current_clause><legal_description><examples><part_of_prompt_to_modify>\"\n","                \"<intro> is: Consider the following online terms of service clause:\"\n","                \"current_clause> is the variable clause to be classified.\"\n","                \"<legal_description> is: question helping to classify clauses\"\n","                \"<examples> is: optional, example clauses that are either positive or negative to the classification\"\n","                f\"<part_of_prompt_to_modify> is {current_part_of_prompt_to_modify}, only modify this part\\n\\n\"\n","                f\"Optimization History: {self.optimization_history}\\n\\n\"\n","                \"Choose the most suitable action to further improve the prompt based on the score trends:\\n\"\n","                \"- shorten: Remove unnecessary details.\\n\"\n","                \"- add_positive_example: Include a positive example.\\n\"\n","                \"- add_negative_example: Include a negative example.\\n\"\n","                \"- remove_positive_example: Remove the existing positive example.\\n\"\n","                \"- remove_negative_example: Remove the existing negative example.\\n\"                \n","                \"- paraphrase: Rephrase the prompt to be more clear and concise.\\n\"\n","                \"- reformat: Change the structure or formatting of the prompt.\\n\"\n","                \"- grammar_adjustment: Fix any grammatical errors in the prompt.\\n\"\n","            )\n","\n","        # 2. Get LLM's Decision\n","        response = model(llm_prompt)  \n","        chosen_action = response.lower().strip()\n","\n","        \n","        # 3. Apply Chosen Action\n","        if chosen_action == \"shorten\":\n","            part_of_prompt_to_modify = shorten_prompt(part_of_prompt_to_modify)\n","        elif chosen_action == \"add_positive_example\":\n","            part_of_prompt_to_modify = add_positive_example(part_of_prompt_to_modify,category)\n","        elif chosen_action == \"add_negative_example\":\n","            part_of_prompt_to_modify = add_negative_example(part_of_prompt_to_modify,category)\n","        elif chosen_action == \"remove_positive_example\":\n","            part_of_prompt_to_modify = remove_positive_example(part_of_prompt_to_modify,category)\n","        elif chosen_action == \"remove_negative_example\":\n","             part_of_prompt_to_modify = remove_negative_example(part_of_prompt_to_modify,category)            \n","        elif chosen_action == \"paraphrase\":\n","            paraphraser = ParaphrasePegasus() \n","            part_of_prompt_to_modify = paraphraser.paraphrase_pegasus(part_of_prompt_to_modify)\n","        elif chosen_action == \"reformat\":\n","            part_of_prompt_to_modify = reformat(part_of_prompt_to_modify)\n","        elif chosen_action == \"grammar_adjustment\":\n","            grammarAdjustment = GrammarAdjustment()\n","            part_of_prompt_to_modify = grammarAdjustment.grammar_adjustment(current_prompt)\n","        else:\n","            raise ValueError(f\"Invalid action chosen by LLM: {chosen_action}\")\n","\n","        # 4. Score and Log (using self.optimization_history)\n","        score = score_prompt(part_of_prompt_to_modify)\n","        self.optimization_history.append(\n","            {\"iteration\": iteration + 1, \"prompt\": part_of_prompt_to_modify, \"edit\": chosen_action, \"score\": score}  # Log 1-based iteration\n","        )\n","\n","    return self.optimization_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define your model (using your actual API key/setup)\n","import json\n","from typing import Dict, List\n","\n","from langchain_groq import ChatGroq\n","from langchain.schema import HumanMessage, SystemMessage\n","\n","model = ChatGroq(\n","    model_name=\"llama3-70b-8192\",\n","    groq_api_key=\"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\",\n","    temperature=0\n",")\n","\n","initial_prompt = make_answer_instruction()\n","\n","# Initial prompt to be optimized\n","#initial_prompt = \"Consider the following online terms of service clause: 'websites & communications terms of use'. Does this clause describe an arbitration dispute resolution process that is not fully optional to the consumer? Begin your answer with 'yes' or 'no' and then justify your response in no more than 50 words. For example, consider these example clauses: <positive, if you are not a consumer in the eea , the exclusive place of jurisdiction for all disputes arising from or in connection with this agreement is san francisco county , california , or the united states district court for the northern district of california and our dispute will bedetermined under california law .'>, <negative, you are prohibited from using any services or facilities provided in connection with this service to compromise security or tamper with system resources and/or accounts .>\"\n","\n","#cat_results = {}\n","#df_train_sample = sample_equal_distribution(df_train, unfairness_categories, 100, 123)\n","#score_set_df = sample_equal_distribution(df_train, unfairness_categories, 25, 234)\n","#...\n","\n","for cat in unfairness_categories:\n","    \n","   # Run prompt optimization, specifying we're on the 4th and last iteration\n","   optimized_history = run_prompt_optimization(\n","        initial_prompt,\n","        model,\n","        num_iterations=4,\n","        category=cat\n","    )\n","   optimized_prompt = optimized_history[-1][\"prompt\"]\n","    \n","\n","print(json.dumps(optimized_history, indent=2))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
